{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_data.json', 'r', encoding='UTF-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# validation data\n",
    "# max participants: 10\n",
    "# max turns: 20\n",
    "\n",
    "# train data\n",
    "# max participants: 10\n",
    "# max turns: 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit number of participants (max 2)\n",
    "for i in range(len(data['data'])- 1, -1, -1):\n",
    "    num_parts = data['data'][i]['header']['dialogueInfo']['numberOfParticipants']\n",
    "    if num_parts > 2:\n",
    "        del data['data'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove special characters (except question mark)\n",
    "special_characters_pattern = r\"[^\\w\\s?]\"\n",
    "def preprocess_text(text):\n",
    "    processed_text = re.sub(special_characters_pattern, '', text)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge turns\n",
    "# remove characters consisting only of consonants or vowels\n",
    "# remove unnecessary columns\n",
    "for i in range(len(data['data'])):\n",
    "    del data['data'][i]['header']\n",
    "    item = data['data'][i]\n",
    "    merged_utter = {}\n",
    "    for utter in item['body']:\n",
    "        if utter['turnID'] not in merged_utter:\n",
    "            merged_utter[utter['turnID']] = utter['utterance']\n",
    "        else:\n",
    "            merged_utter[utter['turnID']] += ' ' + utter['utterance']\n",
    "    for j in range(len(item['body']) - 1, -1, -1):\n",
    "        turn_ID = item['body'][j]['turnID']\n",
    "        if turn_ID in merged_utter:\n",
    "            item['body'][j]['utterance'] = merged_utter[turn_ID]\n",
    "            item['body'][j]['utterance'] = preprocess_text(re.sub('([ㄱ-ㅎㅏ-ㅣ]+)','',item['body'][j]['utterance']))\n",
    "            del merged_utter[turn_ID]\n",
    "            del item['body'][j]['utteranceID']\n",
    "            del item['body'][j]['turnID']\n",
    "            del item['body'][j]['date']\n",
    "            del item['body'][j]['time']\n",
    "        else:\n",
    "            del item['body'][j]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'body': [{'participantID': 'P01', 'utterance': '나지금밥머거2시간걸어서 번화가찾았어 잉'},\n",
       "  {'participantID': 'P02',\n",
       "   'utterance': '헐  언넝호텔들가 엄청피건할첸데 나는인낫러요 나 두시출근이다 퀵으로한대서 두시까지오래 '},\n",
       "  {'participantID': 'P01', 'utterance': '오좋겠네'},\n",
       "  {'participantID': 'P02', 'utterance': '잘잣어??'},\n",
       "  {'participantID': 'P01', 'utterance': '아니 자지도못했어 진짜피씨방에서30분? 너가좋아하는돈가쭈 머거'},\n",
       "  {'participantID': 'P02', 'utterance': '잉 나도줘 내돈가쓰'},\n",
       "  {'participantID': 'P01', 'utterance': '맛있어 고로케도존맛탱'},\n",
       "  {'participantID': 'P02', 'utterance': '사진찍엇어???'},\n",
       "  {'participantID': 'P01', 'utterance': ' 먼사진?'},\n",
       "  {'participantID': 'P02', 'utterance': '돈따스'},\n",
       "  {'participantID': 'P01', 'utterance': '안보내줫어? 엥보낸줄알았는대 시스템사진 이거'},\n",
       "  {'participantID': 'P02', 'utterance': '하 '}]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate subset of turns\n",
    "data_copy = data['data'][:]\n",
    "for item in data_copy:\n",
    "    for i in range(3, len(item['body'])+1):\n",
    "        subset = item['body'][:i]\n",
    "        data['data'].append({'body': subset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample 500000 train data, 50000 test data\n",
    "random_samples = random.sample(data['data'], 500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'w', encoding='UTF-8') as file:\n",
    "    json.dump(random_samples, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
